# -*- coding: utf-8 -*-
"""
Created on Wed Jan  9 11:32:15 2019

@author: Suriani
"""

import numpy as np
import matplotlib.pyplot as plt
import math

###############################################################################
# Bandits
###############################################################################
class Bandits:
    def __init__(self, args):
        # Recebe como entrada um tupla com a esperança de cada bandit
        
        # Transforma a tupla de esperanças em uma lista
        self.m = list(args) 
        self.k = np.size(self.m)    # Quantidade de bandidos
    
    def pull(self, in00):
        # Simula uma recompensa aleatória com esperança m para o bandit in01
        # self.m[in00] = recompensa esperada do bandit
        # np.random.randn() = valor aleatório com média 0 e desvio padrão 1
        # (sigma)*np.random.randn() + media : recompenas com (media, sigma²)
        # max(r, 0) impede recompensas negativas
        med = self.m[in00]      # media
        sigma2 = 1              # variância
        return np.sqrt(sigma2) * np.random.randn() + med
            
###############################################################################
# MyBandit
###############################################################################
class MyBandit:
    def __init__(self, in00, in01=0):  
        # in00: endereço da máquina de bandidos
        # in01: estimador inicial da média de cada bandido
        self.bandits = in00     # Guarda ponteiro para a máquina com bandidos
        k = self.bandits.k      # bandits.k: número de bandidos
        self.mean = [in01]*k    # Estimativa de recompensa (média das recompen-
                                # sas dadas): valor inicial in01 - default 0
        self.N = [0]*k          # Número tentativas realizadas: valor inicial 0
        
    def update(self, in00):     
        # Aciona o bandido in00
        recompensa = self.bandits.pull(in00)
        # Anota que mais uma tentativa foi realizada no bandit in00
        self.N[in00] += 1
        # Atualiza a média do bandit in00 (não armazena recompensas anteriores)
        self.mean[in00] += (1.0/self.N[in00]) * (recompensa - self.mean[in00])
        return recompensa

###############################################################################
# Ploter
###############################################################################
def Ploter(media_acumulada, m, title):
    fig = plt.figure(figsize=(7, 4))
    ax = fig.add_axes([0, 0, 1, 1], xscale='log')
    ax.plot(media_acumulada, label='Média Acumulada')
    N = np.size(media_acumulada)
    for pos, med in enumerate(m):
        ax.plot(np.ones(N)*med, '--', label='m'+str(pos + 1))
    ax.legend(loc=7)
    ax.set_title(title, fontSize=15)
    
    print('\n'+title)
    for pos, med in enumerate(m):
        print('m{} = {:.2f}'.format(str(pos + 1), med))
    
###############################################################################
# Epsilon Greedy
###############################################################################
def experimento_epsgreedy(eps, N, ploter, *args):
    '''
    eps = valor de epsilon para este experimento
    N = número de tentativas do experimento
    ploter = True para imprimir gráfico e cabeçalho, False para suprimir
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    bandits = MyBandit(Bandits(args))
    title = 'Método ϵ Greedy - ϵ = {}'.format(eps)
    
    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)

    for i in range(N):              # Varre as N tentativas
        p = np.random.random()      # Sorteia um valor entre 0 e 1 (uniforme)
        if p < eps:                 # p < ϵ: 
            j = np.random.choice(bandits.bandits.k)     # Escolha aleatória
        else:                       # p >= ϵ: 
            j = np.argmax(bandits.mean)   # Escolhe bandit com maior estimativa
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j)   
    
    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média estimada de cada bandit após as N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    if ploter: Ploter(media_acumulada, bandits.bandits.m, title)

    # Retorna a média acumulada para o usuário
    return media_acumulada

###############################################################################
# Optimistic Initial Values
###############################################################################
def experimento_optimistic(N, lim_sup, ploter, *args):
    '''
    lim_sup = limite superior, ou estimativa inicial de recompensa dos bandits
    N = número de tentativas do experimento
    ploter = True para imprimir gráfico e cabeçalho, False para suprimir
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    bandits = MyBandit(Bandits(args), lim_sup)
    title = 'Método Optimistic Initial Values - Limite Superior = {}'\
        .format(lim_sup)

    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)
    
    for i in range(N):                 # Varre as N tentativas
        j = np.argmax(bandits.mean)    # Escolhe o bandit com maior média atual
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j)   
    
    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média esperada de cada bandit após N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    if ploter: Ploter(media_acumulada, bandits.bandits.m, title)
    
    # Retorna a média acumulada para o usuário
    return media_acumulada

###############################################################################
# UCB: Upper Confidence Bound
###############################################################################
def ucb(media, N, Nj):
    if Nj==0:
        return float('inf')
    else:
        return media + (2*math.log(N) / Nj)**0.5

def experimento_ucb(N, ploter, *args):
    '''
    N = número de tentativas do experimento
    ploter = True para imprimir gráfico e cabeçalho, False para suprimir
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    bandits = MyBandit( Bandits(args) )
    title = 'Método UCB - Upper Confidence Bound'
    
    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)
    
    for i in range(N):                 # Varre as N tentativas
        # Escolhe o bandit com maior ucb atual
        j = np.argmax([ucb(m, i+1, n) for m, n in zip(bandits.mean, 
                                                      bandits.N)])   
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j)   
    
    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média esperada de cada bandit após N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    if ploter: Ploter(media_acumulada, bandits.bandits.m, title)
    
    # Retorna a média acumulada para o usuário
    return media_acumulada

###############################################################################
# Decaying Epsilon Greedy
###############################################################################
def experimento_deg(N, ploter, *args):
    '''
    N = número de tentativas do experimento
    ploter = True para imprimir gráfico e cabeçalho, False para suprimir
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    bandits = bandits = MyBandit(Bandits(args))
    title = 'Decaying Epsilon Greedy'
    
    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)
    
    for i in range(N):              # Varre as N tentativas
        p = np.random.random()      # Sorteia um valor entre 0 e 1 (uniforme)
        if p < 1.0/(1.0+i):         # p < 1/(1 + tentativa): 
            j = np.random.choice(bandits.bandits.k)     # Escolha aleatória
        else:                       # p >= 1/(1 + tentativa): 
            j = np.argmax(bandits.mean)   # Escolhe bandit com maior estimativa
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j) 
    
    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média esperada de cada bandit após N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    if ploter: Ploter(media_acumulada, bandits.bandits.m, title)
    
    # Retorna a média acumulada para o usuário
    return media_acumulada