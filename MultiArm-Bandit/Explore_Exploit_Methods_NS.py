# -*- coding: utf-8 -*-
"""
Created on Wed Jan  9 11:32:25 2019

@author: Suriani
"""

import numpy as np
import matplotlib.pyplot as plt

###############################################################################
# Bandits
###############################################################################
class Bandits:
    def __init__(self, args):
        # Recebe como entrada um tupla com a esperança de cada bandit
        
        # Transforma a tupla de esperanças em uma lista
        self.m = list(args) 
        self.k = np.size(self.m)    # Quantidade de bandidos
    
    def pull(self, in00):
        # Simula uma recompensa aleatória com esperança m para o bandit in01
        # self.m[in00] = recompensa esperada do bandit
        # np.random.randn() = valor aleatório com média 0 e desvio padrão 1
        # (sigma)*np.random.randn() + media : recompenas com (media, sigma²)
        # sigma² = 0.5 * media
        # max(r, 0) impede recompensas negativas
        med = self.m[in00]      # media
        sigma2 = 0.5 * med      # variância
        return max((np.sqrt(sigma2)) * np.random.randn() + med, 0)

class NSBandits:
    def __init__(self, args):
        # args: primeiro valor: número de passos do valor inicial ao final
        # args: restante: tupla com recompensas inicial e final de cada 
        # bandido (i1, f1, i2, f2, i3, f3,...)
        self.mi = []        # Lista com recompensas iniciais
        self.m = []         # Lista com recompensas finais
        for pos, num in enumerate(args):
            if pos == 0:
                self.steps = num        # Número de passos Não Estacionarios
            elif pos%2:
                self.mi.append(num)     # Lista com recompensas iniciais
            else:
                self.m.append(num)      # Lista com recompensas finais
        self.k = np.size(self.m)        # Quantidade de bandidos
        self.pulls = 0                  # Vezes que a máquina foi acionada
                                        # Valor Inicial: zero p/ cd bandit
    
    def pull(self, in00):
        # Simula uma recompensa aleatória com esperança m para o bandit in00
        # m = minimo( (tentativa/#passos), 1 ) * (mf - mi) + mi
        # Tentativa = 0: m = mi
        # Tentativa >= #passos: m = mf
        # np.random.randn() = valor aleatório com média 0 e desvio padrão 1
        # (sigma)*np.random.randn() + media : recompenas com (media, sigma²)
        # sigma² = 0.5 * media
        # max(r, 0) impede recompensas negativas
        med = min(self.pulls/self.steps, 1) *\
            (self.m[in00] - self.mi[in00]) +\
            self.mi[in00]       # media
        sigma2 = 0.5 * med      # variância    
        self.pulls += 1   # Incrementa 1 ao número de acionamentos   
        return max((np.sqrt(sigma2)) * np.random.randn() + med, 0)
            
###############################################################################
# MyBandit
###############################################################################
class MyBandit:
    def __init__(self, in00, in01=0):  
        # in00: endereço da máquina de bandidos
        # in01: estimador inicial da média de cada bandido
        self.bandits = in00     # Guarda ponteiro para a máquina com bandidos
        k = self.bandits.k      # bandits.k: número de bandidos
        self.mean = [in01]*k    # Estimativa de recompensa (média das recompen-
                                # sas dadas): valor inicial in01 - default 0
        self.N = [0]*k          # Número tentativas realizadas: valor inicial 0
        
    def update(self, in00):     
        # Aciona o bandido in00
        recompensa = self.bandits.pull(in00)
        # Anota que mais uma tentativa foi realizada no bandit in00
        self.N[in00] += 1
        # Atualiza a média do bandit in00 (não armazena recompensas anteriores)
        self.mean[in00] += (1.0/self.N[in00]) * (recompensa - self.mean[in00])
        return recompensa
    
class MyBanditNS:               # NonStationary
    def __init__(self, in00, in01): 
        # in00: endereço da máquina de bandidos
        # in01: alpha ou parâmetro step-size
        self.bandits = in00     # Guarda ponteiro para a máquina com bandidos
        k = self.bandits.k      # bandits.k: número de bandidos
        self.mean = [0]*k       # Estimativa de recompensa (média das recompen-
                                # sas dadas): valor inicial 0
        self.alpha = in01       # Alpha
        
    def update(self, in00):     
        # Aciona o bandido in00
        recompensa = self.bandits.pull(in00)
        # Atualiza a média do bandit in00 (não armazena recompensas anteriores)
        self.mean[in00] += self.alpha * (recompensa - self.mean[in00])
        return recompensa

###############################################################################
# Ploter
###############################################################################
def ploter(media_acumulada, m, title):
    fig = plt.figure(figsize=(7, 4))
    ax = fig.add_axes([0, 0, 1, 1], xscale='log')
    ax.plot(media_acumulada, label='Média Acumulada')
    N = np.size(media_acumulada)
    for pos, med in enumerate(m):
        ax.plot(np.ones(N)*med, '--', label='m'+str(pos + 1))
    ax.legend(loc=7)
    ax.set_title(title, fontSize=15)
    
    print('\n'+title)
    for pos, med in enumerate(m):
        print('m{} = {:.2f}'.format(str(pos + 1), med))
    
###############################################################################
# Epsilon Greedy
###############################################################################
def experimento_epsgreedy(eps, N, Stationary, *args):
    '''
    eps = valor de epsilon para este experimento
    N = número de tentativas do experimento
    Stationary = True para Stationary e False para NonStationary
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    if Stationary:
        bandits = MyBandit(Bandits(args))
        title = 'Método ϵ Greedy - ϵ = {} - Stationary Bandits'.format(eps)
    else:
        bandits = MyBandit(NSBandits(args))
        title = 'Método ϵ Greedy - ϵ = {} - NonStatio. Bandits'.format(eps)
    
    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)

    for i in range(N):              # Varre as N tentativas
        p = np.random.random()      # Sorteia um valor entre 0 e 1 (uniforme)
        if p < eps:                 # p < ϵ: 
            j = np.random.choice(bandits.bandits.k)     # Escolha aleatória
        else:                       # p >= ϵ: 
            j = np.argmax(bandits.mean)   # Escolhe bandit com maior estimativa
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j)   
    
    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média estimada de cada bandit após as N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    ploter(media_acumulada, bandits.bandits.m, title)

    # Retorna a média acumulada para o usuário
    return media_acumulada

###############################################################################
# Epsilon Greedy NonStationary (alpha)
###############################################################################
def experimento_epsgreedyNS(eps, N, alpha, Stationary, *args):
    '''
    eps = valor de epsilon para este experimento
    N = número de tentativas do experimento
    alpha = parâmetro step-size (0, 1]
    Stationary = True para Stationary e False para NonStationary
    args = entradas dos bandidos (recompensas espeadas ou "reais")
    '''
    
    # Inicializa bandits
    if Stationary:
        bandits = MyBanditNS(Bandits(args), alpha)
        title = 'Método ϵ Greedy e Parâmetro α - ' +\
            'ϵ = {}; α = {} - Stationary Bandits'.format(eps, alpha)
    else:
        bandits = MyBanditNS(NSBandits(args), alpha)
        title = 'Método ϵ Greedy e Parâmetro α - ' +\
            'ϵ = {}; α = {} - NonStatio. Bandits'.format(eps, alpha)
    
    # Inicializa um vetor que conterá as recompenas de cada tentativa
    recompensas = np.empty(N)
    
    for i in range(N):              # Varre as N tentativas
        p = np.random.random()      # Sorteia um valor entre 0 e 1 (uniforme)
        if p < eps:                 # p < ϵ: 
            j = np.random.choice(bandits.bandits.k)     # Escolha aleatória
        else:                       # p >= ϵ: 
            j = np.argmax(bandits.mean)   # Escolhe bandit com maior estimativa
        # Puxa o j-ésimo bandit e guarda a recompensa obtida em um vetor
        recompensas[i] = bandits.update(j) 

    # Calcula e plota a média acumulada das recompensas.
    # Imprime a média estimada de cada bandit após N tentativas
    media_acumulada = np.cumsum(recompensas)/(np.arange(N) + 1)
    ploter(media_acumulada, bandits.bandits.m, title)

    # Retorna a média acumulada para o usuário
    return media_acumulada